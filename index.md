---
layout: default
---
This website hosts the upcoming tutorial series for advanced NLP methods, for computational social science scholars.

Every few weeks, we will host some experts in the field of computational social science to present a new method in NLP, and to lead participants in an interactive exploration of the method with code and sample text data.
If you are a graduate student or researcher who has some introductory knowledge of NLP (e.g. has learned text analysis from [SICSS](https://sicss.io/curriculum)) and wants to "level up", come join us!

Watch past tutorials on [our YouTube channel](https://www.youtube.com/channel/UCcFcF9DkanjaK3HEk7bsd-A). 

## Tutorial format

- **Introduction**: the facilitators introduce their method and the code/data associated with the method.
- **Interaction**: the participants break out into small groups to test out the method using the provided code. The code includes several spots for experimenting with the method, which can help the participants better understand the benefits and limitations of the method. The participants may also bring their own data for analysis if desired.
- **Conclusion**: the facilitators bring all participants together to collect their experiences with the method and to share final thoughts on possible improvements on the method.

Tutorials will last one hour and we encourage participants to join live. However, we will also make the recordings and code publicly available afterwards.


## Logistics on joining tutorials live   

We will send out the tutorial video link to our mailing list a few days before the tutorial starts.
If you want to join the mailing list, subscribe [here](https://groups.google.com/g/nlp-css-tutorials).

## Schedule

**Note**: all dates are tentative; we will send the final date/time for each tutorial on the mailing list prior to the tutorial.
All times are in US EST timezone (UTC-4) unless otherwise noted.

| Date | Title | Description | Leader | Background info |
|---|---|:--|---|---|
| 4/29 10:30 AM [RSVP](https://forms.gle/4atQE7HyGHy8Y9xP9) | Processing Code-mixed Text  | Code-mixing, i.e., the mixing of two or more languages in a single utterance or conversation, is an extremely common phenomenon in multilingual societies. It is amply present in user-generated text, especially in social media. Therefore, CSS research that handles such text requires to process code-mixing; there are also interesting CSS and socio-linguistic questions around the phenomenon of code-mixing itself. In this tutorial, we will equip you with some basic tools and techniques for processing code-mixed text, starting with hands-on experiments with word-level language identification, all the way up to methods for building code-mixed text classifiers using massively multilingual language models. | [Monojit Choudhury](https://www.microsoft.com/en-us/research/people/monojitc/), [Sanad Rizvi](https://github.com/mohdsanadzakirizvi) | [MBERT](https://aclanthology.org/N19-1423.pdf) |
| 5/9 | Word Embeddings for Descriptive Corpus Analysis: Digging Deeper into Analogies, Polysemy, and Stability | Word embeddings such as word2vec have recently garnered attention as potentially useful tools for analysis in social science. They promise an unsupervised method  to quantify the connotations of words, and compare these across time or different subgroups. However, when training or using word embeddings, researchers may find that they don’t work as well as expected, or produce unreplicable results.  We focus on three subtle issues in their use that could result in misleading observations: (1) indiscriminate use of analogical reasoning, which has been shown to underperform on many types of analogies; (2) the surprising prevalence of polysemous words and distributional similarity of antonyms, both leading to counterintuitive results; and (3) instability in nearest-neighbor distances caused by sensitivity to noise in the training process.  Through demonstrations, we will learn how to detect, understand, and most importantly mitigate the effects of these issues. | [Neha Kennard](https://nnkennard.github.io/) | N/A |


## Archive

See below for links to materials for previous tutorials.

| Tutorial | Description | Leader | Links |
|---|---|:--|---|
| Comparing Word Embedding Models| We'll demonstrate an extension of the use of word embedding models by fitting multiple models on a social science corpus (using gensim's word2vec implementation), then aligning and comparing those models. This method is used to explore group variation and temporal change. We'll discuss some tradeoffs and possible extensions of this approach.| [Connor Gilroy](https://ccgilroy.com/), [Sandeep Soni](https://sandeepsoni.github.io/) | [Code](https://colab.research.google.com/drive/16cM5NXedlrvU2mp-HcYKs9OIMkYItTS1?usp=sharing); [Video](https://youtu.be/WbzPZZKJRJA) |
| Extracting Information from Documents| This workshop provides an introduction to information extraction for social science–techniques for identifying specific words, phrases, or pieces of information contained within documents. It focuses on two common techniques, named entity recognition and dependency parses, and shows how they can provide useful descriptive data about the civil war in Syria. The workshop uses the Python library spaCy, but no previous experience is needed beyond familiarity with Python.| [Andrew Halterman](https://www.andrewhalterman.com/)| [Code](https://colab.research.google.com/drive/1U6x-3OVCGtx9CBZvzdJi8mhTxCx8k4Ie?usp=sharing); [Video](https://youtu.be/sUtthdcPyhc) |
| Controlling for Text in Causal Inference with Double Machine Learning| Establishing causal relationships is a fundamental goal of scientific research. Text plays an increasingly important role in the study of causal relationships across domains especially for observational (non-experimental) data. Specifically, text can serve as a valuable "control" to eliminate the effects of variables that threaten the validity of the causal inference process. But how does one control for text, an unstructured and nebulous quantity? In this tutorial, we will learn about bias from confounding, motivation for using text as a proxy for confounders, apply a "double machine learning" framework that uses text to remove confounding bias, and compare this framework with non-causal text dimensionality reduction alternatives such as topic modeling. | [Emaad Manzoor](https://emaadmanzoor.com/)| [Code](https://colab.research.google.com/drive/15Jz9QehJsT2um1cH5GEbbBOeJDcX0e2n?usp=sharing); [Video](https://youtu.be/DwUqA1ydJI0N); [Slides](docs/NLP+CSS201 - Controlling for Text in Causal Inference.pdf) |
| Beyond the Bag Of Words: Text Analysis with Contextualized Topic Models | Most topic models still use Bag-Of-Words (BoW) document representations as input. These representations, though, disregard the syntactic and semantic relationships among the words in a document, the two main linguistic avenues to coherent text. Recently, pre-trained contextualized embeddings have enabled exciting new results in several NLP tasks, mapping a sentence to a vector representation. Contextualized Topic Models (CTM) combine contextualized embeddings with neural topic models to increase the quality of the topics. Moreover, using multilingual embeddings allows the model to learn topics in one language and predict them for documents in unseen languages, thus addressing a task of zero-shot cross-lingual topic modeling.|  [Silvia Terragni](https://silviatti.github.io/)| [Code](https://colab.research.google.com/drive/1EO0bS0Wow_cjdGsDfV38xt--AxJQjFtg?usp=sharing); [Video](https://www.youtube.com/watch?v=n1_G8K07KoM) |
| BERT for Computational Social Scientists| What is BERT? How do you use it? What kinds of computational social science projects would BERT be most useful for? Join for a conceptual overview of this popular natural language processing (NLP) model as well as a hands-on, code-based tutorial that demonstrates how to train and fine-tune a BERT model using HuggingFace's popular Python library.| [Maria Antoniak](https://maria-antoniak.github.io/)| [Code](https://colab.research.google.com/drive/1ih6ETBCU2Dqr1_aTPgjS_Ww3xXVswIO0?usp=sharing); [Video](https://youtu.be/UmyOhl9AciI); [Slides](https://docs.google.com/presentation/d/1HGWnLkv7_2fST9tFVbvQbY-rN4aTMjJW/) |
| Moving from words to phrases when doing NLP| Most people starting out with NLP think of text in terms of single-word units called "unigrams." But many concepts in documents can't be represented by single words. For instance, the single words "New" and "York" can't really represent the concept "New York." In this tutorial, you'll get hands-on practice using the phrasemachine package and the Phrase-BERT model to 1) extract multi-word expressions from a corpus of U.S. Supreme Court arguments and 2) use such phrases for downstream analysis tasks, such as analyzing the use of phrases among different groups or describing latent topics from a corpus.| [Abe Handler](https://www.abehandler.com/), [Shufan Wang](https://people.cs.umass.edu/~shufanwang/) | [Code1](https://colab.research.google.com/drive/17-fsmsfc-5llUksIJZiPPYh_nCYcwBww?usp=sharing); [Code2](https://colab.research.google.com/drive/1neTWJoDkhO2mCKRgqG1klVIkU5Qk3fQh?usp=sharing); [Slides1](https://docs.google.com/presentation/d/1C5O0EdgM33SO1KlCk90rW1g7nuf2264XcuYPZB-b7bI/edit?usp=sharing); [Slides2](https://docs.google.com/presentation/d/1Wg4Txobmxaev0jYXRpCndwtq2IJfcLGXrJsdb0pebbc/edit?usp=sharing); [Video](https://www.youtube.com/watch?v=OgYXtg0ht6s)|
| Analyzing Conversations in Python Using ConvoKit | ConvoKit is a Python toolkit for analyzing conversational data. It implements a number of conversational analysis methods and algorithms spanning from classical NLP techniques to the latest cutting edge, and also offers a database of conversational corpora in a standardized format. This tutorial will walk through an example of how to use ConvoKit, starting from loading a conversational corpus and building up to running several analyses and visualizations. | [Jonathan Chang](https://www.cs.cornell.edu/~jpchang/) | [Code](https://colab.research.google.com/drive/1_jvL1t9PA2dERKbEm9pCnBS0sbW7B1AW?usp=sharing); [Video](https://youtu.be/IZG0iv7bdyM) | 
| Preprocessing Social Media Text | 🤔 hmm howwww should we think about our #NLProc preprocessing pipeline when it comes to informal TEXT written by social media users?!? In this tutorial, we'll discuss some interesting features of social media text data and how we can think about handling them when doing computational text analyses. We will introduce some Python libraries and code that you can use to process text and give you a chance to experiment with some real data from platforms like Twitter and Reddit. | [Steve Wilson](https://steverw.com/) | [Code](https://colab.research.google.com/drive/1hWAd9NFPEyDhdJQotpAF4dfJyr1Ucfap?usp=sharing); [Video](https://www.youtube.com/watch?v=o5XbbZt7oWs) |
| Aggregated Classification Pipelines: Propagating Probabilistic Assumptions from Start to Finish | NLP has helped massively scale-up previously small-scale content analyses. Many social scientists train NLP classifiers and then measure social constructs (e.g sentiment) for millions of unlabeled documents which are then used as variables in downstream causal analyses. However, there are many points when one can make hard (non-probabilistic) or soft (probabilistic) assumptions in pipelines that use text classifiers: (a) adjudicating training labels from multiple annotators, (b) training supervised classifiers, and (c) aggregating individual-level classifications at inference time. In practice, propagating these hard versus soft choices down the pipeline can dramatically change the values of final social measurements.  In this tutorial, we will walk through data and Python code of a real-world social science research pipeline that uses NLP classifiers to infer many users’ aggregate “moral outrage” expression on Twitter. Along the way, we will quantify the sensitivity of our pipeline to these hard versus soft choices.  | [Katherine Keith](kakeith.github.io) |  [Code](https://colab.research.google.com/drive/1ulQSwlSlWTEglzBGVQXKstueIaX5Gm1f?usp=sharing); [Video](https://www.youtube.com/watch?v=snBaBuXKiDI); [Slides](https://docs.google.com/presentation/d/1I7aN5k6tRu6RXiehrAvWPDN5Fv8G56_IEZHmKhZoi9Y/edit?usp=sharing)|
| Estimating causal effects of aspects of language with noisy proxies | Does the politeness of an email or a complaint affect how quickly someone responds to it? This question requires a causal inference: how quickly would someone have responded to an email had it not been polite? With observational data, causal inference requires ruling out all the other reasons why polite emails might be correlated with fast responses. To complicate matters, aspects of language such as politeness are not labeled in observed datasets. Instead, we typically use lexicons or trained classifiers to predict these properties for each text, creating a (probably noisy) proxy of the linguistic aspect of interest. In this talk, I'll first review the challenges of causal inference from observational data. Then, I'll use the motivating example of politeness and response times to highlight the specific challenges to causal inference introduced by working with text and noisy proxies. Next, I'll introduce recent results that establish assumptions and a methodology under which valid causal inference is possible. Finally, I'll demonstrate this methodology: we'll use semi-synthetic data and adapt a text representation method to recover causal effect estimates. | [Dhanya Sridhar](https://www.dsridhar.com/) | [Code](https://colab.research.google.com/drive/101rhkpnQInEkyPysdEmZhF2oZvEavmi6?usp=sharing); [Video](https://www.youtube.com/watch?v=InNTARvDqTM); [Slides](docs/sridhar_umsi_nlp_tutorial.pdf); |

## Hosts

This tutorial series is organized by:

<img src="https://ianbstewart.github.io/docs/istewart.jpg" alt="ian_pic" width="200"/>
- [Ian Stewart](https://ianbstewart.github.io): post-doctoral fellow at University of Michigan; researches personalization and writing interventions for social applications

<img src="https://kakeith.github.io/images/kk.jpg" alt="katie_pic" width="200"/>
- [Katie Keith](https://kakeith.github.io/): post-doctoral researcher at AI2 and incoming Assistant Professor at Williams College (Fall 2022); researches causal inference with text and text-based social data science. 


## Acknowledgments 
We are deeply grateful for financial assistance from a [Social Science Research Council (SSRC)](https://www.ssrc.org/)/[Summer Institutes in Computational Social Science (SICSS)](https://sicss.io/) Research Grant.

### Theme

TBD
